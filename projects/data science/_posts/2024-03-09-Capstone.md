---
layout: post
title: "Climate Bench Plus"
sitemap: false
hide_last_modified: true
permalink: /projects/data science/Capstone/
related_posts:
    -
sitemap: false
image: \assets\img/climate.png
---

# Emulating The Effects of Climate Change with Deep Learning 
For my senior capstone project at UCSD, I, along with two other students, aimed to extend the work of our professor ([Duncan Watson-Parris](https://duncanwp.github.io/)) and his paper [ClimateBench](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954) in the area of climate emulation. He had created ClimateBench, a benchmarking framework that leverages data from a set of *Coupled Model Intercomparison Projects (CMIPS)*, *AerChemMip* and *Detection-Attrition Model Intercomparison Projects* which are collections of physically-based simulations performed from existing **Earth System Models (ESM)**, which could then be used to compare the performances of more lightweight climate emulators built from machine learning algorithms.

Our extension looked to improve the baseline machine learning algorithms that Professor Duncan had used in the creation of ClimateBench (**Gaussian Process**, **Convolutional Neural Network**, **Random Forest**) in order to potentially discover machine learning solutions that were more accurate to the results of the existing ESMs.

## Improved models

### Gaussian Process DKL
To improve the Gaussian Process model, a hybrid model that combines the **Gaussian Process** model with a **Neural Network** resulted in the **Deep Kernel Learning** model. This model works by using the descriptive capabilities of the **Neural Network** to learn a feature representation of the data which then gets translated into a kernel function that the **Gaussian Process** can use to make predictions.

### Physically Informed Neural Network (PINN)
The improvement made to the original CNN was to add the idea of physical constraints to the learning process of the model. This physically informed neural network uses a CNN architechture and works by incorporating physical equations into the loss function, which I took from [FaIRv2.0.0](https://gmd.copernicus.org/articles/14/3007/2021/#abstract). This was the main model I worked on

### XGBoost
Finally in place of the Random Forest we implemented a Gradient Boosting model called XGBoost, a more advanced form of the Random Forest model. XGBoost creates a series of decision trees much like the Random Forest, but further incorporates gradient boosting in order to learn from the mistakes of previous trees.

## Deliverables for the Extension
For a more in-depth look at the models and the project as a whole, below are the deliverables from the project. 

- For a more in-depth technical look at the project, download the [Report](\assets\Capstone/report.pdf).
- For the code of the project, visit the [Github Repository](https://github.com/jackljk/ClimateBench-Plus).
- For a more visual representation and more high-level overview of the project, visit the [Website](https://jackljk.github.io/DSC180B-website/).
- To view the poster that was used for the poster presentation, download the [Poster](\assets\Capstone/poster.pdf).